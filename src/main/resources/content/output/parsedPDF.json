{
  "text": "***START OF PAGE 1***\nProceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 985–992\nManchester, August 2008\nExtractive Summarization Using Supervised and Semi-supervised\nLearning\nKam-Fai Wong*, Mingli Wu*†\n*Department of Systems Engineering and\nEngineering Management\nThe Chinese University of Hong Kong\nNew Territories, Hong Kong\n{kfwong,mlwu}@se.cuhk.edu.hk\nWenjie Li†\n†Department of Computing\nThe Hong Kong Polytechnic University\nKowloon, Hong Kong\ncswjli@comp.polyu.edu.hk\nAbstract\nIt is difficult to identify sentence impor-\ntance from a single point of view. In this\npaper, we propose a learning-based ap-\nproach to combine various sentence fea-\ntures. They are categorized as surface,\ncontent, relevance and event features.\nSurface features are related to extrinsic\naspects of a sentence. Content features\nmeasure a sentence based on content-\nconveying words. Event features repre-\nsent sentences by events they contained.\nRelevance features evaluate a sentence\nfrom its relatedness with other sentences.\nExperiments show that the combined fea-\ntures improved summarization perform-\nance significantly. Although the evalua-\ntion results are encouraging, supervised\nlearning approach requires much labeled\ndata. Therefore we investigate co-training\nby combining labeled and unlabeled data.\nExperiments show that this semi-\nsupervised learning approach achieves\ncomparable performance to its supervised\ncounterpart and saves about half of the\nlabeling time cost.\n1 Introduction\n1 Automatic text summarization involves con-\ndensing a document or a document set to produce\na human comprehensible summary. Two kinds of\nsummarization approaches were suggested in the\npast, i.e., extractive (Radev et al., 2004; Li et al.,\n2006) and abstractive summarization (Dejong,\n1978). The abstractive approaches typically need\n© 2008. Licensed under the Creative Commons Attri-\nbution-Noncommercial-Share Alike 3.0 Unported\nlicense (http://creativecommons.org/licenses/by-nc-\nsa/3.0/). Some rights reserved.\nto “understand” and then paraphrase the salient\nconcepts across documents. Due to the limita-\ntions in natural language processing technology,\nabstractive approaches are restricted to specific\ndomains. In contrast, extractive approaches\ncommonly select sentences that contain the most\nsignificant concepts in the documents. These ap-\nproaches tend to be more practical.\nRecently various effective sentence features\nhave been proposed for extractive summarization,\nsuch as signature word, event and sentence rele-\nvance. Although encouraging results have been\nreported, most of these features are investigated\nindividually. We argue that it is ineffective to\nidentify sentence importance from a single point\nof view. Each sentence feature has its unique\ncontribution, and combing them would be advan-\ntageous. Therefore we investigate combined sen-\ntence features for extractive summarization. To\ndetermine weights of different features, we em-\nploy a supervised learning framework to identify\nhow likely a sentence is important. Some re-\nsearchers explored learning based summarization,\nbut the new emerging features are not concerned,\nsuch as event features (Li et. al, 2006).\nWe investigate the effectiveness of different\nsentence features with supervised learning to de-\ncide which sentences are important for summari-\nzation. After feature vectors of sentences are ex-\namined, a supervised learning classifier is then\nemployed.  Particularly, considering the length of\nfinal summaries is fixed, candidate sentences are\nre-ranked. Finally, the top sentences are ex-\ntracted to compile the final summaries. Experi-\nments show that combined features improve\nsummarization performance significantly.\nOur supervised learning approach generates\npromising results based on combined features.\nHowever, it requires much labeled data. As this\nprocedure is time consuming and costly, we in-\nvestigate semi-supervised learning to combine\nlabeled data and unlabeled data. A semi-\n***END OF PAGE 1***\n\n***START OF PAGE 2***\nsupervised learning classifier is used instead of a\nsupervised one in our extractive summarization\nframework. Two classifiers are co-trained itera-\ntively to exploit unlabeled data. In each iteration\nstep, the unlabeled training examples with top\nclassifying confidence are included in the labeled\ntraining set, and the two classifiers are trained on\nthe new training data. Experiments show that the\nperformance of our semi-supervised learning\napproach is comparable to its supervised learning\ncounterpart and it can reduce the labeling time\ncost by 50%.\nThe remainder of this paper is organized as\nfollows. Section 2 gives related work and Section\n3 describes our learning-based extractive summa-\nrization framework. Section 4 outlines the vari-\nous sentence features and Section 5 describes\nsupervised/semi-supervised learning approaches.\nSection 6 presents experiments and results. Fi-\nnally, Section 7 concludes the paper.\n2 Related Work\nTraditionally, features for summarization were\nstudied separately. Radev et al. (2004) reported\nthat position and length are useful surface fea-\ntures. They observed that sentences located at the\ndocument head most likely contained important\ninformation. Recently, content features were also\nwell studied, including centroid (Radev et al.,\n2004), signature terms (Lin and Hovy, 2000) and\nhigh frequency words (Nenkova e t al., 2006).\nRadev et al. (2004) defined centroid words as\nthose whose average tf*idf score were higher\nthan a threshold. Lin and Hovy (2000) identified\nsignature terms that were strongly associated\nwith documents based on statistics measures.\nNenkova et al. (2006) later reported that high\nfrequency words were crucial in reflecting the\nfocus of the document.\nBag of words is somewhat loose and omits\nstructural information. Document structure is\nanother possible feature for summarization. Bar-\nzilay and Elhadad (1997) constructed lexical\nchains and extracted strong chains in summaries.\nMarcu (1997) parsed documents as rhetorical\ntrees and identified important sentences based on\nthe trees. However, only moderate results were\nreported. On the other hand, Dejong (1978) rep-\nresented documents using predefined templates.\nThe procedure to create and fill the templates\nwas time consuming and it was hard to adapt the\nmethod to different domains.\nRecently, semi-structure events (Filatovia and\nHatzivassiloglou, 2004; Li et al., 2006; Wu, 2006)\nhave been investigated by many researchers as\nthey balanced document representation with\nwords and structures. They defined events as\nverbs (or action nouns) plus the associated\nnamed entities. For instance, given the sentence\n“Yasser Arafat on Tuesday accused the United\nStates of threatening to kill PLO officials”, they\nfirst identified “accused”, “threatening” and\n“kill” as event terms; and “Yasser Arafat”,\n“United States”, “PLO” and “Tuesday” as event\nelements. Encouraging results based on events\nwere reported for news stories.\nFrom another point of view, sentences in a\ndocument are somehow connected. Sentence\nrelevance has been used as an alternative means\nto identify important sentences. Erkan and Radev\n(2004) and Yoshioka (2004) evaluate the rele-\nvance (similarity) between any two sentences\nfirst. Then a web analysis approach, PageRank,\nwas used to select important sentences from a\nsentence map built on relevance. Promising re-\nsults were reported. However, the combination of\nthese features is not well studied. Wu et al. (2007)\nconducted preliminary research on this problem,\nbut event features were not considered.\nNormally labeling procedure in supervised\nlearning is very time consuming. Blum and\nMitchell (1998) proposed co-training approach to\nexploit labeled and unlabeled data. Promising\nresults were reported from their experiments on\nweb page classification. A number of successful\nstudies emerged thereafter for other natural lan-\nguage processing tasks, such as text classification\n(Denis and Gilleron, 2003), noun phrase chunk-\ning (Pierce and Cardie, 2001), parsing (Sarkar,\n2001) and reference or relation resolution (Mul-\nler et al., 2001; Li et al., 2004). To our knowl-\nedge, there is little research in the application of\nco-training techniques to extractive summariza-\ntion.\n3 The Framework for Extractive Sum-\nmarization\nExtractive summarization can be regarded as a\nclassification problem. Given the features of a\nsentence, a machine-learning based classification\nmodel will judge how likely the sentence is im-\nportant. The classification model can be super-\nvised or semi-supervised learning. Supervised\napproaches normally perform better, but require\nmore labeled training data. SVMs perform well\nin many classification problems. Thus we em-\nploy it for supervised learning. For semi-\nsupervised learning, we co-trained a probabilistic\n***END OF PAGE 2***\n\n***START OF PAGE 3***\nSVM and a Naïve Bayesian classifier to exploit\nunlabeled data.\nFigure 1. Learning-based Extractive Summariza-\ntion Framework\nThe automatic summarization procedure is\nshown in Figure 1. First, each input sentence is\nexamined by going through the pre-specified fea-\nture functions. The classification model will then\npredict the importance of each sentence accord-\ning to its feature values. A re-ranking algorithm\nis then used to revise the order. Finally, the top\nsentences are included in the summaries until the\nlength limitation is reached. The re-ranking algo-\nrithm is crucial, as more important content are\nexpected to be contained in the final summary\nwith fixed length. Important sentences above a\nthreshold are regarded as candidates. The one\nwith less words and located at the beginning part\nof a document is ranked first. The re-ranking al-\ngorithm is described as follows.\nRanki = RankPosi + RankLengthi\nwhere RankPosi is the rank of sentence i accord-\ning to its position in a document (i.e. the sentence\nno.) and RankLengthi is rank of sentence i ac-\ncording to its length.\n4 Sentence Features for Extractive\nSummarization\nThis section provides a detailed description on\nthe four types of sentence features, i.e., surface,\ncontent, event and relevance features, which will\nbe examined systematically.\n4.1 Surface Features\nSurface features are based on structure of\ndocuments or sentences, including sentence\nposition in the document, the number of words in\nthe sentence, and the number of quoted words in\nthe sentence (see Table 1).\nName Description\nPosition 1/sentence no.\nDoc_First Whether it is the first sentence of a\ndocument\nPara_First Whether it is the first sentence of a\nparagraph\nLength The number of words in a sentence\nQuote The number of quoted words in a sen-\ntence\nTable 1. Types of surface features\nThe intuition with respect to the importance of\na sentence stems from the following observations:\n(1) the first sentence in a document or a para-\ngraph is important; (2) the sentences in the ear-\nlier parts of a document is more important than\nsentences in later parts; (3) a sentence is impor-\ntant if the number of words (except stop words)\nin it is within a certain range; (4) a sentence con-\ntaining too many quoted words is unimportant.\n4.2 Content Features\nWe integrate three well-known sentence features\nbased on content-bearing words i.e., centroid\nwords, signature terms, and high frequency\nwords. Both unigram and bigram representations\nhave been investigated. Table 2 summarizes the\nsix content features we studied.\nName Description\nCentroid_Uni The sum of  the weights of cen-\ntroid uni-gram\nCentroid_Bi The sum of  the weights of cen-\ntroid bi-grams\nSigTerm_Uni The number of signature uni-\ngrams\nSigTerm_Bi The number of signature bi-grams\nFreqWord_Uni The sum of  the weights of fre-\nquent uni-grams\nFreqWord_Bi The sum of  the weights of fre-\nquent bi-grams\nTable 2. Types of content features\n4.3 Event Features\nAn event is comprised of an event term and asso-\nciated event elements. In this study, we choose\nverbs (such as “elect and incorporate”) and ac-\ntion nouns (such as “election and incorporation”)\nas event terms that can characterize actions. They\nrelate to “did what”. One or more associated\nnamed entities are considered as event elements.\nFour types of named entities are currently under\n***END OF PAGE 3***\n\n***START OF PAGE 4***\nconsideration. The GATE system (Cunningham\net al., 2002) is used to tag named entities, which\nare categorized as <Person>, <Organization>,\n<Location> and <Date>. They convey the infor-\nmation about “who”, “whom”, “when” and\n“where”. A verb or an action noun is deemed an\nevent term only when it appears at least once\nbetween two named entities.\nEvent summarization approaches based on in-\nstances or concepts are investigated. An occur-\nrence of an event term (or event element) in a\ndocument is considered as an instance, while the\ncollection of the same event terms (or event ele-\nments) is considered as a concept. Given a\ndocument set, instances of event terms and event\nelements are identified first. An event map is\nthen built based on event instances or concepts\n(Wu , 2006; Li et al., 2006). PageRank algorithm\nis used to assign weight to each node (an instance\nor concept) in the event map. The final weight of\na sentence is the sum of weights of event in-\nstances contained in the sentence.\n4.4 Relevance Features\nRelevance features are incorporated to exploit\ninter-sentence relationships. It is assumed that: (1)\nsentences related to important sentences are im-\nportant; (2) sentences related to many other sen-\ntences are important. The first sentence in a\ndocument or a paragraph is important, and other\nsentences in a document are compared with the\nleading ones. Two types of sentence relevance,\nFirstRel_Doc and FirstRel_Para (see Table 3),\nare measured by comparing pairs of sentences\nusing word-based cosine similarity.\nAnother way to exploit sentence relevance is\nto build a sentence map. Every two sentences are\nregarded relevant if their similarity is above a\nthreshold. Every two relevant sentences are con-\nnected with a unidirectional link. Based on this\nmap, PageRank algorithm is applied to evaluate\nthe importance of a sentence. These relevance\nfeatures are shown in Table 3.\nName Description\nFirstRel_Doc Similarity with the first sentence in\nthe document\nFirstRel_Para Similarity with the first sentence in\nthe paragraph\nPageRankRel PageRank value of the sentence\nbased on the sentence map\nTable 3. Types of relevance features\n5 Supervised/Semi-supervised Learning\nApproaches\nTo incorporate features described in Section 4,\nwe investigate supervised and semi-supervised\nlearning approaches. Probabilistic Support Vec-\ntor Machine (PSVM)  is employed as supervised\nlearning (Wu et al., 2004), while the co-training\nof PSVM and Naïve Bayesian Classifier (NBC)\nis used for semi-supervised learning. The two\nlearning-based classification approaches, PSVM\nand NBC, are described in following sections.\n5.1 Probabilistic Support Vector Machine\n(PSVM)\nFor a set of training examples ( ix , iy ),\nli ,...,1= , where ix  is an instance and iy  the\ncorresponding label, basic SVM requires the so-\nlution of the following optimization problem.\n1  min ξ\nsubject to\ni bxwy\nHere the SVM classifier is expected to find a\nhyper-plane to separate testing examples as posi-\ntive and negative. Wu et al. (2004) extend the\nbasic SVM to a probabilistic version. Its goal is\nto estimate\nkixiyppi ,...1 ),|( === .\nFirst the pairwise (one-against-one) probabilities\n) ,or  |( xjiyiyprij ==≈  is estimated using\nBAfij e\nwhere A and B are estimated by minimizing the\nnegative log-likelihood function using training\ndata and their decision values f. Then ip  is ob-\ntained by solving the following optimization\nproblem\ni ijj\njijijip\nprpr\n1  min\nsubject to\ni bxwy\nThe problem can be reformulated as\nQPP T\n1  min\n***END OF PAGE 4***\n\n***START OF PAGE 5***\nwhere\n= ∑ ≠\nji if\nji if\nijji\nsiiss\nThe problem is convex and the optimality condi-\ntions a scalar b such that\nwhere e is the vector of all 1s and z is the vector\nof all 0s, and b is the Lagrangian multiplier of the\nequality constraint ∑\n5.2 Naïve Bayesian Classier (NBC)\nNaïve Bayesian Classier assumes features are\nindependent. It learns prior probability and con-\nditional probability of each feature, and predicts\nthe class label by highest posterior probability.\nGiven a feature vector (F1, F2, F3,…, Fn), the\nclassifier need to decide the label c:\n),...,,|(maxarg 321 n\nFFFFcPc =\nBy applying Bayesian rule, we have\n),...,,,(\n)|,...,,,()(\n),...,,,|(\nn FFFFP\ncFFFFPcP\nFFFFcP =\nSince the denominator does not depend on c and\nthe values of Fi are given, therefore the denomi-\nnator is a constant and we are only interested in\nthe numerator. As features are assumed inde-\npendent,\ncFPcP\ncFFFFPcPFFFFcP\n)|()(\n)|,...,,()(),...,,|(\nwhere )|( cFP i is estimated with MLE from\ntraining data with Laplace Smoothing.\n5.3 Co-Training (COT)\nSupervised learning approaches require much\nlabeled data and the labeling procedure is very\ntime-consuming. Literature (Blum and Mitchell,\n1998; Collins, 1999) has suggested that unla-\nbeled data can be exploited together with labeled\ndata by co-training two classifiers. (Blum and\nMitchell, 1998) trained two classifiers of same\ntype on different features, and (Li et al., 2004)\ntrained two classifiers of different types. In this\npaper, as the number of involved features is not\ntoo many, we train two different classifiers,\nPSVM and NBC, on the same feature spaces.\nThe co-training algorithm is described as follows.\nGiven:\nL is the set of labeled training examples\nU is the set of unlabeled training examples\nLoop: until the unlabeled data is exhausted\nTrain the first classifier C1 (PSVM) on L\nTrain the second classifier C2 (NBC) on L\nFor each classifier Ci\nCi labels examples from U\nCi chooses p positive and n negative ex-\namples E from U. These examples have\ntop classifying confidence.\nCi removes examples E from U\nCi adds examples E with the correspond-\ning labels to L\nOutput: label the test examples by the optimal\nclassifier which is evaluated on training data ac-\ncording to the classification performance.\n6 Experiments\nDUC 20012 has been used in our experiments. It\ncontains 30 clusters of relevant documents and\n308 documents in total. Each cluster deals with a\nspecific topic (e.g. a hurricane) and comes with\nmodel summaries created by NIST assessors. 50,\n100, 200 and 400 word summaries are provided.\nTwenty-five of the thirty document clusters are\nused as training data and the remaining five are\nused as testing. The training/testing configuration\nis same in experiments of supervised learning\nand semi-supervised learning, while the differ-\nence is that some sentences in training data are\nnot tagged for semi-supervised learning.\nAn automatic evaluation package, i.e.,\nROUGE (Lin and Hovy, 2003) is employed to\nevaluate the summarization performance. It\ncompares machine-generated summaries with\nmodel summaries based on the overlap. Precision\nand recall measures are used to evaluate the clas-\nsification performance. For comparison, we\nevaluate our approaches on DUC 2004 data set\nalso. It contains 50 clusters of documents. Only\n665-character summaries are given by assessors\nfor each cluster.\n6.1 Experiments on Supervised Learning\nApproach\nWe use LibSVM3 as our classification model for\nSVM classifiers normally perform better. Types\nof features presented in previous section are\nevaluated individually first. Precision measures\n2 http://duc.nist.gov/\n3 http://www.csie.ntu.edu.tw/~cjlin/libsvm/\n***END OF PAGE 5***\n\n***START OF PAGE 6***\nthe percentage of true important sentences\namong all important sentences labeled by the\nclassifier. Recall measures the percentage of true\nimportant sentences labeled by the classifier\namong all true important sentences.\nTable 4 shows the precisions and recalls of\ndifferent feature groups under the PSVM classi-\nfier. Table 5 records the ROUGE evaluation re-\nsults – ROUGE-1, ROUGE-2 and ROUGE-L.\nThey evaluate the overlap between machine-\ngenerated summaries and model summaries\nbased on unigram, bigram and long distance re-\nspectively. The summary length is limited to 200\nwords here.\nFeature Precision Recall\nSur+Event 0.600 0.125\nCon+Event 0.384 0.194\nRel+Event 0.543 0.132\nSur+Con+Event 0.595 0.153\nSur+Rel+Event 0.553 0.146\nCon+Rel+Event 0.581 0.125\nSur+Con+Rel 0.595 0.174\nSur+Con+Rel+Event 0.579 0.153\nTable 4. Classification performance based on\ndifferent feature groups\nFeature Rouge-\nRouge-\nRouge-\nSur+Con+Rel+Event 0.375 0.106 0.359\nTable 5. ROUGE evaluation results for differ-\nent feature groups\nFrom Table 4, we can see the most useful fea-\nture groups are “surface” and “relevance”, i.e.\nthe external characteristics of a sentence in the\ndocument and the relationships of a sentence\nwith other sentences in a cluster. The evaluation\nscores from surface features and relevance fea-\ntures are the same. We found that the reason is\nthat the dominating feature in each feature group\nis about whether a sentence is the first sentence\nin a document. The influence of event features is\nnot very positive. Based on our analysis the rea-\nson is that not all clusters contain enough event\nterms/elements to build a good event map.\nFrom Table 5, it can be seen that the combina-\ntion of multiple features or multiple feature\ngroups outperforms individual feature or feature\ngroups. When surface, content and relevance fea-\ntures are employed, the best performance is\nachieved, i.e., ROUGE-1 and ROUGE-2 score\nare 0.396 and 0.116 respectively. In our prelimi-\nnary experiments, we find ROUGE-1 score of a\nmodel summary is 0.422 (without stemming and\nfiltering stop words). Therefore summaries gen-\nerated by our supervised learning approach re-\nceived comparable performance with model\nsummaries when evaluated by ROUGE. Al-\nthough ROUGE is not perfect at this time, it is\nautomatic and good complement to subjective\nevaluations.\nWe also find that the Rouge scores are similar\nfor variations on the feature set. Sentences from\noriginal documents are selected to build the final\nsummaries. Normally, only four to six sentences\nare contained in one 200-word summary in our\nexperiments, i.e., few sentences will be kept in a\nsummary. As variations of the feature set only\ninduce little change of the order of most impor-\ntant sentences, the ROUGE scores change little.\n6.2 Experiments on Semi-supervised Learn-\ning Approach\nSupervised learning approaches normally\nachieve good performance but require manually\nlabeled data. Recent literature (Blum and\nMitchell, 1998; Collins, 199) has suggested that\nco-training techniques reduce the amount of la-\nbeled data. They trained two homogeneous clas-\nsifiers based on different feature spaces. How-\never this method is unsuitable for our application\nas the number of required features in our case is\nnot too many. Therefore we develop a co-\ntraining approach to train different classifiers\nbased on same feature space. PSVM and NBC\nare applied to the combination of surface, content\nand relevance features.\nThe capability of different learning approaches\nto identify important sentences is shown in Fig-\n***END OF PAGE 6***\n\n***START OF PAGE 7***\nure 2. The “x” axis shows the number of labeled\nsentences employed. The remained training sen-\ntences in DUC 2001 are employed as unlabeled\ntraining data. The y axis shows f-measures of\nimportant sentences identified from the test set.\nThe size of the training seed set is investigated.\nFor each size, three different seed sets which are\nchose randomly are used. The average evaluation\nscores are used as the final performance. This\nprocedure avoids the variance of the final evalua-\ntion results. The ROUGE evaluation results of\nthese supervised learning approaches and semi-\nsupervised learning approaches are shown in Ta-\nble 6 (2000 labeled sentences). It can be seen that\nthe ROUGE performance of co-trained classifiers\nis better than that of individual classifiers.\nNumber of Labeled Sentences\nre Cotrain\nBayes\nFigure 2. Performance of supervised learning\nand semi-supervised learning approaches\nLearning\nApproaches Rouge-1 Rouge-2 Rouge-L\nTable 6. ROUGE evaluation results of supervised\nlearning and semi-supervised learning\n6.3 Experiments on Summary Length\nIn DUC 2001 dataset, 50, 100, 200 and 400-word\nsummaries are provided to evaluate summaries\nwith different length. Our supervised approach,\nwhich generates the best performance in previous\nexperiments, is employed. The ROUGE scores of\nevaluations on different summary length are\nshown in Table 7. Our summaries consist of ex-\ntracted sentences. It can be seen that these sum-\nmaries achieve lower ROUGE scores when the\nlength of summary is reduced. The reason is that\nwhen people try to write a more concise sum-\nmary, condensed contents are included in the\nsummaries, which may not use the original con-\ntents directly. Therefore the word-overlapping\ntest tool in ROUGE generates lower scores.\nWe then tested the same classifier and same\nfeatures on DUC 2004. The length of summaries\nis only 665 characters (about 100 words).\nROUGE-1 and ROUGE-2 are 0.329 and 0.073\nrespectively. It confirms that the performance of\nour approach is sensitive to the length of the\nsummary.\nSum_length Rouge-1 Rouge-2 Rouge-L\nTable 7. ROUGE evaluation results for differ-\nent summary length\n7 Conclusions and Future Work\nWe explore surface, content, event, relevance\nfeatures and their combinations for extractive\nsummarization with supervised learning ap-\nproach. Experiments show that the combination\nof surface, content and relevance features per-\nform best. The highest ROUGE-1, ROUGE-2\nscores are 0.396 and 0.116 respectively. The\nRouge-1 score of manually generated summaries\nis 0.422. This shows the ROUGE performance of\nour supervised learning approach is comparable\nto that of manually generated summaries. The\nROUGE-1 scores of extractive summarization\nbased on centroid, signature word, high fre-\nquency word and event individually are 0.319,\n0.356, 0.371 and 0.374 respectively. It can be\nseen that our summarization approach based on\ncombination of features improves the perform-\nance obviously.\nAlthough the results of supervised learning\napproach are encouraging, it required much la-\nbeled data. To reduce labeling cost, we apply co-\ntraining to combine labeled and unlabeled data.\nExperiments show that compare with supervised\nlearning, semi-supervised learning approach\nsaves half of the labeling cost and maintains\ncomparable performance (0.366 vs 0.396). We\nalso find that our extractive summarization is\nsensitive to length of the summary. When the\nlength is extended, the ROUGE scores of same\nsummarization method are improved. In the fu-\nture, we plan to investigate sentence compression\nto improve performance of our summarization\napproaches on short summaries.\nAcknowledgement\nThe research described in this paper is partially\nsupported by Research Grants Council of Hong\nKong (RGC: PolyU5217/07E), CUHK Strategic\nGrant Scheme (No: 4410001) and Direct Grant\nScheme (No: 2050417).\n***END OF PAGE 7***\n\n***START OF PAGE 8***\nReferences\nRegina Barzilay, and Michael Elhadad. 1997. Using\nlexical chains for text summarization. In Proceed-\nings of the 35th Annual Meeting of the Association\nfor Computational Linguistics Workshop on Intel-\nligent Scalable Text Summarization, pages 10-17.\nAvrim Blum and Tom Mitchell. 1998. Combining\nlabeled and unlabeled data with co-training. In\nProceedings of the 11th Annual Conference on\nComputational Learning Theory, pages 92-100.\nHamish Cunningham, Diana Maynard, Kalina\nBontcheva, Valentin Tablan. 2002. GATE: a\nframework and graphical development environ-\nment for robust NLP tools and applications. In\nProceedings of the 40th Annual Meeting of the As-\nsociation for computational Linguistics.\nFrancois Denis and Remi Gilleron. 2003. Text classi-\nfication and co-training from positive and unla-\nbeled examples. In Proceedings of the 20th Inter-\nnational Conference on Machine Learning Work-\nshop: the Continuum from Labeled Data to Unla-\nbeled Data in Machine Learning and Data Mining.\nGunes Erkan and Dragomir R. Radev. 2004. LexPag-\neRank: prestige in multi-document text summariza-\ntion. In Proceedings of the 2004 Conference on\nEmpirical Methods in Natural Language Process-\ning, pages 365-371.\nElena Filatova and Vasileios Hatzivassiloglou. Event-\nbased extractive summarization. 2004. In Proceed-\nings of the 42nd Annual Meeting of the Association\nfor Computational Linguistics Workshop, pages\nGerald Francis DeJong. 1978. Fast skimming of news\nstories: the FRUMP system. Ph.D. thesis, Yale\nUniversity.\nWenjie Li, Guihong Cao, Kam-Fai Wong and Chunfa\nYuan. 2004. Applying machine learning to Chinese\ntemporal relation resolution. In Proceedings of the\n42nd Annual Meeting of the Association for Com-\nputational Linguistics, pages 583-589.\nWenjie Li, Wei Xu, Mingli Wu, Chunfa Yuan, Qin Lu.\n2006. Extractive summarization using inter- and in-\ntra- event relevance. In proceedings of Proceedings\nof the 21st International Conference on Computa-\ntional Linguistics and 44th Annual Meeting of the\nAssociation for Computational Linguistics, pages\nChin-Yew Lin; Eduard Hovy. 2000. The automated\nacquisition of topic signatures for text summariza-\ntion. In Proceedings of the 18th International Con-\nference on Computational Linguistics, pages 495-\nChin-Yew Lin and Eduard Hovy. 2003. Automatic\nevaluation of summaries using n-gram co-\noccurrence statistics. In Proceedings of the 2003\nHuman Language Technology Conference of the\nNorth American Chapter of the Association for\nComputational Linguistics, Edmonton, Canada.\nDaniel Marcu. 1997. The rhetorical parsing of natural\nlanguage texts. In Proceedings of the 35th Annual\nMeeting of the Association for computational Lin-\nguistics, pages 96-103.\nChristoph Muller, Stefan Rapp and Michael Strube.\n2001. Applying co-training to reference resolution.\nIn Proceedings of the 40th Annual Meeting on As-\nsociation for Computational Linguistics.\nAni Nenkova, Lucy Vanderwende and Kathleen\nMcKeown. 2006. A compositional context sensi-\ntive multi-document summarizer: exploring the\nfactors that influence summarization. In Proceed-\nings of the 29th Annual International ACM SIGIR\nConference on Research and Development in In-\nformation Retrieval.\nDavid Pierce and Claire Cardie. 2001. Limitations of\nco-training for natural language learning from large\ndatasets. In Proceedings of the 2001 Conference on\nEmpirical Methods in Natural Language Process-\ning, pages 1-9.\nDragomir R. Radev, Timothy Allison, et al. 2004.\nMEAD - a platform for multidocument multilin-\ngual text summarization. In Proceedings of 4th In-\nternational Conference on Language Resources\nand Evaluation.\nAnoop Sarkar. 2001. Applying co-training methods to\nstatistical parsing. In Proceedings of 2nd Meeting\nof the North American Chapter of the Association\nfor Computational Linguistics on Language Tech-\nnologies.\nMingli Wu. 2006. Investigations on event-based\nsummarization. In proceedings of the 21st Interna-\ntional Conference on Computational Linguistics\nand 44th Annual Meeting of the Association for\nComputational Linguistics Student Research Work-\nshop, pages 37-42.\nMingli Wu, Wenjie Li, Furu Wei, Qin Lu and Kam-\nFai Wong. 2007. Exploiting surface, content and\nrelevance features for learning-based extractive\nsummarization. In Proceedings of 2007 IEEE In-\nternational Conference on Natural Language\nProcessing and Knowledge Engineering.\nTing-Fan Wu, Chih-Jen Lin and Ruby C. Weng. 2004.\nProbability estimates for multi-class classification\nby pairwise coupling. Journal of Machine Learning\nResearch, 5:975-1005.\nMasaharu Yoshioka and Makoto Haraguchi. 2004.\nMultiple news articles summarization based on\nevent reference information. In Working Notes of\nthe Fourth NTCIR Workshop Meeting, National In-\nstitute of Informatics.\n***END OF PAGE 8***\n\n"
}